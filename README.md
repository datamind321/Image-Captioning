# Image Captioning Using Vision Transformers
Example: 
![img](https://github.com/datamind321/Image-Captioning/blob/main/Screenshot%202024-07-17%20011648.png)
![img](https://github.com/datamind321/Image-Captioning/blob/main/Screenshot%202024-07-17%20011546.png)
This repository contains a project that explores the task of image captioning using Vision Transformers (ViTs). The project aims to generate descriptive captions for images by combining the power of Transformers and computer vision. It leverages state-of-the-art pre-trained ViT models and employs techniques such as attention mechanisms and language modeling to generate accurate and contextually relevant captions.
# Introduction 
Image captioning is a challenging problem that involves generating human-like descriptions for images. By utilizing Vision Transformers, this project aims to achieve improved image understanding and caption generation. The combination of computer vision and Transformers has shown promising results in various natural language processing tasks, and this project explores their application to image captioning.
# Method Used 
The following methods and techniques are employed in this project:

- Vision Transformers (ViTs)
- Attention mechanisms
- Language modeling
- Transfer learning
- Evaluation metrics for image captioning (e.g., BLEU, METEOR, CIDEr)

# Technologies
The project is implemented in Python and utilizes the following libraries:

- PyTorch
- Transformers
- TorchVision
- NumPy
- NLTK
- Matplotlib

  Link to Blog: https://www.analyticsvidhya.com/blog/2023/06/vision-transformers/
